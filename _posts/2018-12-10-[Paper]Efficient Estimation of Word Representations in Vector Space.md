---
    author: LuckyGong
    comments: true
    date: 2018-12-10 20:27
    layout: post
    title: Paper-Efficient Estimation of Word Representations in Vector Space
    categories:
    - nlp
    tags:
    - nlp
    - paper
---

# Abstract

- 提出两种算法。
- 在单词相似性任务中度量算法质量，与不同得神经网络比较，发现我们的算法成本更低，准确度更高。

# 1.Introduction

- 许多当前的NLP系统和技术将单词视为原子单元 - 单词之间没有相似性概念，因为它们在词汇表中表示为索引。这样的有点：简单、鲁棒、易观察。对大量数据进行训练的简单模型优于使用较少数据训练的复杂模型。
- 有人在做单词的分布式表示，基于神经网络的语言模型由于N-gram模型。



## 1.1 文章目标

- 本文的主要目标是介绍可用于从数十亿字的大数据集中学习高质量单词向量的技术，以及词汇表中数百万字的单词。 
- 据我们所知，以前提出的架构都没有成功地训练超过几亿个单词，单词向量的适度维度在50-100之间。
- 在这篇文章中，我们尝试开发新的框架，它能够保存单词之间存在的线性规律，进而尝试最大化向量操作的精确度；我们设计新的复杂测试集用来句法和语义规则测试；而且，我们讨论训练时间和精确度与单词向量维数、训练数据的大小之间的关系

## 1.2之前的工作

- NNLM：时间复杂度Q = N × D + N × D × H + H × V

# 2.网络模型

- 现在有许多不同模型来估计单词的连续表示，如LSA、LDA，本文关注神经网络学习的单词的分布式表示。根据经验，其表现会优于LSA；LDA在大数据集上计算成本高，不予考虑。
- 目的：最大化准确度，最小化计算复杂度（定义为需要访问以完全训练模型的参数数量）。
- 对于以下所有模型，训练复杂度与之成正比：O = E × T × Q ，E是epoch（一般是3-50）、T是训练集词的数目（一般高达一百万）、Q是模型架构定义，用sgd训练。

## 2.1NNLM

- 组成：
  - 输入层：1-V编码对前N个word进行编码，V是词表词数
  - 映射层P：共享参数矩阵N * D，仅N个输入是激活的。
  - 隐藏层：
  - 输出层（输出V）：
- 总计算复杂度：Q = N * D + N * D * H + H * V。隐藏层（H * V）的计算量特别大。V可能是500-2000， H通常为500-1000。

- 复杂度降低：
  - 使用分层softmax。
  - 避免对模型进行归一化，转而在训练的时候使用未归一化的模型。
  - 词汇表|V|存在哈夫曼树中：
    - 采用词表的二叉树表示，可以将输出单元的数量降低到log2(V)。至此模型的主要复杂计算就在于第二项 N * D * H了。
    - 霍夫曼树将短二进制代码分配给出现的频繁的单词，这进一步减少了需要评估的输出单元的数量。例如，当词汇量大小为一百万字时，这导致评估的速度增加约两倍。

## 2.2RNNLM

- 基于RNN克服某些前馈NNLM的某些限制：
  - 不需要制定上下文长度。
  - 理论上RNN可以有效地表示比浅层神经元更复杂的模式。
  - 这允许循环模型形成某种短期记忆。
- 组成：输入、隐藏、输出层。
- 总计算复杂度：Q = H × H + H × V 。单词与隐藏层H都有相同的维度。
- 复杂度降低：词汇表|V|存在哈夫曼树中，至此模型的主要复杂计算就在于H × H了。

# 3.新的对数线性模型

- 大多数计算复杂性是由模型中的非线性隐藏层引起的。

- 首先，使用简单模型学习连续单词向量，然后是N-gram NNLM在这些之上进行了训练分布式的单词表示。

## 3.1Continuous Bag-of-Words Model 

- 非线性隐藏层被移除，投影层被共享用于所有单词（不仅仅共享投影矩阵）。
- 所有单词都被投射到相同的位置（它们的向量被平均而不是concat）。我们将这种架构称为词袋模型，因为历史中词语的顺序不会影响投影。而且，我们还用未来的单词。

- 复杂度：Q = N × D + D × log2(V )
- 图示：

## 3.2Continuous Skip-gram Model 

- 基于同一句子中的另一个单词来最大化单词的分类

- 我们使用每个当前单词作为具有连续投影层的对数线性分类器的输入，并预测当前单词之前和之后的特定范围内的单词。 
- 我们发现增加范围可以提高生成的单词向量的质量，但也会增加计算复杂度。 由于较远的单词通常与当前单词的相关性较低，因此我们通过在训练示例中对这些单词进行较少的抽样来减少对远程单词的权重。

- 复杂度：Q= C × (D + D × log2(V )) 。C是词的最远距离。如果C=5，则在每一轮训练中，随机选R∈<1,5>作为窗口，选当前word的前R个word、后R个word共2R个word。