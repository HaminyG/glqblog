---
    author: LuckyGong
    comments: true
    date: 2018-12-10 20:27
    layout: post
    title: Paper-Efficient Estimation of Word Representations in Vector Space
    categories:
    - nlp
    tags:
    - nlp
    - paper
---

# Abstract

- 提出两种算法。
- 在单词相似性任务中度量算法质量，与不同得神经网络比较，发现我们的算法成本更低，准确度更高。

# 1.Introduction

- 许多当前的NLP系统和技术将单词视为原子单元 - 单词之间没有相似性概念，因为它们在词汇表中表示为索引。这样的有点：简单、鲁棒、易观察。对大量数据进行训练的简单模型优于使用较少数据训练的复杂模型。
- 有人在做单词的分布式表示，基于神经网络的语言模型由于N-gram模型。



## 1.1 文章目标

- 本文的主要目标是介绍可用于从数十亿字的大数据集中学习高质量单词向量的技术，以及词汇表中数百万字的单词。 
- 据我们所知，以前提出的架构都没有成功地训练超过几亿个单词，单词向量的适度维度在50-100之间。
- 在这篇文章中，我们尝试开发新的框架，它能够保存单词之间存在的线性规律，进而尝试最大化向量操作的精确度；我们设计新的复杂测试集用来句法和语义规则测试；而且，我们讨论训练时间和精确度与单词向量维数、训练数据的大小之间的关系

## 1.2之前的工作

- NNLM：时间复杂度Q = N × D + N × D × H + H × V

# 2.网络模型

- 现在有许多不同模型来估计单词的连续表示，如LSA、LDA，本文关注神经网络学习的单词的分布式表示。根据经验，其表现会优于LSA；LDA在大数据集上计算成本高，不予考虑。
- 目的：最大化准确度，最小化计算复杂度（定义为需要访问以完全训练模型的参数数量）。
- 对于以下所有模型，训练复杂度与之成正比：O = E × T × Q ，E是epoch（一般是3-50）、T是训练集词的数目（一般高达一百万）、Q是模型架构定义，用sgd训练。

## 2.1NNLM

- 组成：
  - 输入层：1-V编码对前N个word进行编码，V是词表词数
  - 映射层P：共享参数矩阵N * D，仅N个输入是激活的。
  - 隐藏层：
  - 输出层（输出V）：
- 总计算复杂度：Q = N * D + N * D * H + H * V。隐藏层（H * V）的计算量特别大。V可能是500-2000， H通常为500-1000。

- 复杂度降低：
  - 使用分层softmax。
  - 避免对模型进行归一化，转而在训练的时候使用未归一化的模型。
  - 词汇表|V|存在哈夫曼树中：
    - 采用词表的二叉树表示，可以将输出单元的数量降低到log2(V)。至此模型的主要复杂计算就在于第二项 N * D * H了。
    - 霍夫曼树将短二进制代码分配给出现的频繁的单词，这进一步减少了需要评估的输出单元的数量。例如，当词汇量大小为一百万字时，这导致评估的速度增加约两倍。

## 2.2RNNLM

- 基于RNN克服某些前馈NNLM的某些限制：
  - 不需要制定上下文长度。
  - 理论上RNN可以有效地表示比浅层神经元更复杂的模式。
  - 这允许循环模型形成某种短期记忆。
- 组成：输入、隐藏、输出层。
- 总计算复杂度：Q = H × H + H × V 。单词与隐藏层H都有相同的维度。
- 复杂度降低：词汇表|V|存在哈夫曼树中，至此模型的主要复杂计算就在于H × H了。

# 3.新的对数线性模型

- 大多数计算复杂性是由模型中的非线性隐藏层引起的。

- 首先，使用简单模型学习连续单词向量，然后是N-gram NNLM在这些之上进行了训练分布式的单词表示。

## 3.1Continuous Bag-of-Words Model 

- 非线性隐藏层被移除，投影层被共享用于所有单词（不仅仅共享投影矩阵）。
- 所有单词都被投射到相同的位置（它们的向量被平均而不是concat）。我们将这种架构称为词袋模型，因为历史中词语的顺序不会影响投影。而且，我们还用未来的单词。

- 复杂度：Q = N × D + D × log2(V )
- 图示：

## 3.2Continuous Skip-gram Model 

- 基于同一句子中的另一个单词来最大化单词的分类
- 我们使用每个当前单词作为具有连续投影层的对数线性分类器的输入，并预测当前单词之前和之后的特定范围内的单词。 
- 我们发现增加范围可以提高生成的单词向量的质量，但也会增加计算复杂度。 由于较远的单词通常与当前单词的相关性较低，因此我们通过在训练示例中对这些单词进行较少的抽样来减少对远程单词的权重。
- 复杂度：Q= C × (D + D × log2(V )) 。C是词的最远距离。如果C=5，则在每一轮训练中，随机选R∈<1,5>作为窗口，选当前word的前R个word、后R个word共2R个word。

# 4.Results

- 我们根据前面的观察，单词之间可以有许多不同类型的相似性，例如，单词big和bigger在相同的意义上相似，单词small和smaller相似。

- 要找到一个与“big”在相同意义上类似于“small”的词，我们可以简单地计算向量x=向量（“biggest”）—向量（“big”）+向量（“small”）。然后，我们在向量空间中搜索余弦距离测量的最接近x的单词，并将其作为问题的答案。

## 4.1任务描述

- 为了衡量词向量的质量，我们定义了一个包含五类语义问题和九类句法问题的综合测试集。总的来说，有8869个语义问题和10675个句法问题。每个类别中的问题都是通过两个步骤创建的：
  - 首先，手动创建类似单词对的列表。
  - 然后，通过连接两个词对形成一个大的问题列表。例如，我们列出了68个美国大城市及其所属的州，通过随机选择两个词对，形成了大约2.5k的问题。
  - 我们在测试集中只包含单个标记词，因此不存在多词实体（如纽约）
- 我们评估所有问题类型和每个问题类型（语义、句法）的整体准确性。
- 只有当使用上述方法计算出的向量最接近的词与问题中的正确词完全相同时，才能假定问题得到正确回答；同义词被视为错误。这也意味着要达到100%的准确度是不可能的，因为当前的模型没有任何有关词形的输入信息。

## 4.2最大化精度

- 我们使用了谷歌新闻语料库来训练词汇向量。这个语料库包含大约6B个标记。
- 我们已经将词汇表限制在一百万个最常见的词。
- 为了快速评估模型体系结构的最佳选择以获得尽可能好的结果，我们首先对训练数据子集上训练的模型进行了评估，词汇限制在最频繁的30K单词。
- 可以看出，在某一点之后，添加更多维度或添加更多培训数据会减少改进。因此，我们必须同时增加向量维数和训练数据的数量。
- 对于表2和表4中报告的实验，我们使用了三个具有随机梯度下降和反向传播的训练epoch。我们选择起始学习率0.025，并将其线性降低，使其在最后一个训练阶段结束时接近零。

## 4.3模型体系结构比较

- 首先，我们比较了不同的模型结构，用相同的训练数据和词向量的相同维数（640个）来训练词向量。
- 在进一步的实验中，我们在新的语义句法词汇关系测试集中使用了全套问题，即全部的30K词汇。
- 在表3中，可以看到来自RNN的单词向量（如[20]中所用）在句法问题上表现良好。nnlm向量的性能明显优于rnn—这并不奇怪，因为rnnlm中的字向量直接连接到非线性隐藏层。CBOW体系结构在句法任务上比NNLM工作得更好，在语义任务上也差不多。最后，与cbow模型相比，skip-gram架构在句法任务上的效果稍差（但仍优于nnlm），在测试的语义部分比所有其他模型都要好得多。
- 单cpu训练：cbow模型在大约一天的时间内接受了谷歌新闻数据子集的培训，而skip-gram模型的培训时间大约是三天。
- 使用一个epoch对一个模型进行两倍的数据训练，其结果与使用三个epoch对相同的数据进行训练相比具有更好的效果，如表5所示，并且提供了额外的小加速。

## 4.4大规模并行训练

- 我们已经在一个称为distclience的分布式框架中实现了各种模型。
- 这些模型具有小批量异步梯度下降和自适应学习速率过程adagrad
- 我们在培训期间使用了50到100个模型副本。CPU核心的数量是一个估计，因为数据中心机器与其他生产任务共享，并且使用率可能会有很大的波动。

## 4.5 Microsoft Research Sentence Completion Challenge

- 这个任务由1040个句子组成，每个句子中有一个单词缺失，目标是选择与句子其余部分最连贯的单词，给出五个合理的选择列表。
- 已有：n-gram模型、基于LSA的模型、对数双线性模型、目前在这一基准上保持55.4%精确度的最先进性能的RNN模型。
- 我们研究了Skip-Gram体系结构在这项任务中的性能。首先，我们在[32]中提供的50M单词上训练640维词向量模型。然后，我们使用输入的未知单词计算测试集中每个句子的分数，并预测句子中所有周围的单词。最终句子得分就是这些个别预测的和，根据句子得分，我们选择最可能的句子。
- 虽然skip-gram模型本身在这项任务上的表现并不比lsa相似性更好，但该模型的分数与用rnnlms获得的分数是互补的，加权组合导致了一个新的最新结果，准确率为58.9%。

# 5.Examples of the Learned Relationships

- 如表8，例如，巴黎-法国+意大利=罗马。可以看出，准确度是相当好的，尽管显然还有很大的改进空间（请注意，使用我们假设精确匹配的准确度度量，表8中的结果将仅得分60%）。
- 提升：
  - 使用更大数据集、更大维数
  - 是提供关系（Relationship）的多个示例。通过使用10个例子而不是1个来形成关系向量（我们将单个向量平均在一起），我们观察到在语义句法测试中，我们的最佳模型的准确性提高了10%左右。
- 应用：可以应用向量运算来解决不同的任务。例如，通过计算单词列表的平均向量，并找到最远的单词向量，我们发现从列表中选择单词的准确性很好。

# 6.结论

- 我们观察到，与流行的神经网络模型（前馈和循环）相比，使用非常简单的模型结构来训练高质量的字向量是可能的。由于计算复杂度要低得多，所以可以从大得多的数据集中计算非常精确的高维字矢量。
- 使用distisfience分布式框架，可以训练cbow和skip-gram模型，甚至在拥有一万亿个单词的语料库上，基本上不限制词汇表的大小，这比之前公布的同类模型的最佳结果大几个数量级。
- 我们正在进行的工作表明，矢量词可以成功地应用于知识库中事实的自动扩展，也可以用于验证现有事实的正确性，机器翻译实验的结果也很有希望突破。

# 7.后续工作

- 我们发布了单机多线程C++代码，用于计算单词向量，同时使用CBOW和Skip-gram结构，训练速度明显高于本文前面的估计。
- 我们还发布了超过140万个表示命名实体的向量，这些向量经过了1000多亿个单词的训练。