---
author: LuckyGong
comments: true
date: 2018-09-25 22:52:52
layout: post
title: Paper-Factorization Machines
categories:
- ad
tags:
- ad
- paper
---

# Abstract

- 许多预测任务需要对类别特征进行建模（如：ID），一般会对数据onehot，这会使得数据极其稀疏。要从这样的稀疏数据中学习，重要的是要考虑特征之间的相互作用[4, 23, 31]。
- FM考虑特征之间的相互作用，但是只是线性关系，Wide&Deep和DeepCross中则有非线性关系，但是深层结构使其难以训练，结合一波。
- 本文提出了一种NFM，比FM更具表达能力，因为FM可以被看作是NFM没有隐藏层的特殊情况。对两个回归任务的实证结果表明，有一个隐含层时NFM显著优于FM，相对改善率为7:3%。
- 与近年来的Wide&Deep和DeepCross学习方法相比，NFM采用了较浅的结构，但其性能更高，在实践中更容易训练和调整。

# 1.INTRODUCTION

- 已有的技术：
  - 手动变量交叉：由于需要大量的工程技术和有用的领域知识来设计特征，所以这些特征的代价是很高的。不能推广到训练数据中没有出现的组合特征。
  - 设计ML模型来自动地从原始数据中学习特征交互，将高维稀疏特征嵌入到低维潜在中空间，可以推广到看不见的特征组合：
    - FM，性能受其线性性质的限制，难以估计参数。
    - 基于神经网络的非线性模型
- 本文通过建模高阶和非线性特征交互增强FM。通过设计神经网络中的新操作 - 双线性交互（双向交互）pooling（Bilinear Interaction (Bi-Interaction) pooling）——首次在神经网络框架下加入FM。通过在双向交互层（Bi-Interaction layer）上方堆叠非线性层，我们能够加深浅线性FM，有效地建模高阶和非线性特征的相互作用，以提高FM的表现力，双向交互pooling编码更多互相影响的特征（？）。传统方法简单地将低层的embedding向量连接or求平均，我们用双线性交互（双向交互）pooling编码更多交互特征信息，使得后面的层能学习更有意义的信息。
- 本文贡献：
  - 引入双向交互池操作，并为FM提供一种新的神经网络视角。
  - 开发了一种新的NFM模型，用于在神经网络框架下深化FM，用于学习高阶和非线性特征交互。
  - 我们对两个实际任务进行了广泛的实验，研究了双向交互池和NFM模型，证明了NFM的有效性，并且在稀疏的预测中使用神经网络进行预测具有很大的希望。

# 2.现有的特征交互建模

## 2.1FM

- FM开始用于协同推荐。
- 模型详解：见论文。
- 模型优缺点：
  - FM有很强的通用性，与仅仅建模两个实体（entity）关系的矩阵分解相反，FM用于学习任何实值的特征向量。通过指定输入特征，FM可以模仿许多特定的分解模型，如标准MF，并行因子分析和SVD ++。
  - FM试用特征间的二阶分解相互作用来增强LR。
  - 仍然是线性的。
- 模型变体：CoFM、FFM、AFM、

## 2.2DNN

- 面临问题：DNN很少用到IR和DM中来，IR和DM任务的大多数数据稀疏，例如用户行为，文档/查询等。尽管DNN在密集数据中表现出很强的学习能力，但在稀疏数据上的使用却不大行。
- 相关工作：
  - 提出了neural collaborative Filtering (NCF) 框架，后来NCF框架被扩展为属性感知（attribute-aware）CF的模型属性交互（？）。然而，该方法仅适用于学习两个实体之间的交互，并不直接支持监督学习的一般情况。
  - FNN：通过FM学到的特征嵌入来初始化DNN。
  - Wide&Deep：连接嵌入特征向量以学习特征交互。
  - DeepCross：用于广告预测。把Wide&Deep的MLP用先进的残差网络替换。
- 原理：网络结构相似，都是在embedding向量的串联之上堆叠多个层以学习特征交互，期望多层可以隐式方式学习任意阶的组合特征。
- 关键弱点：简单地连接（concat）特征embedding向量带来了太少的低级特征交互的信息。一个研究[16]表明简单地连接用户和item的embedding向量导致协同过滤的结果很差。要解决这个问题，必须加深网络层次来学习有意义的交互。但是深层学习可能又会产生梯度消失/爆炸，这使得模型训练困难，这又引出了模型难训练的问题。
- 解决：我们提出了一种新的Bi-Interaction操作来模拟二阶特征交互，而不是连接特征嵌入向量。进一步探索了使用FM学习的特征嵌入来初始化DNN，这可以被视为一个预训练步骤。

# 3.神经因子机

## 3.1NFM模型

- 公式：其中的的f(x)是网络模型部分，是一个多层NN。

![](https://upload-images.jianshu.io/upload_images/4155986-186da78f1c6e9564.png?imageMogr2/auto-orient/)

- 图示：

  ![](http://kubicode.me/img/Deep-in-out-Wide-n-Deep-Series/nfm_arch.png)

- 组成：

  - embedding层：全连接，将特征投影到稠密向量表示。vi是第i个特征对应的一个实数embedding向量。有：Vx={x1v1,……,xnvn}（其中xi是实数，xi≠0的才能留在这个embedding向量中）。注意：我们用输入特征值调整了embedding向量，这里不仅仅是一个简单的lookup表。

  - Bi-Interaction层：池化操作，将一组embedding向量Vx转化为一个向量。与平均、最大池化相同。

    - 符号：
      - 中间的符号：表示两个向量的对应位元素乘法。
      - v2：v中间的符号v
    - 公式：

    ![](http://ot0qvixbu.bkt.clouddn.com/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180921181045.png)

    - 意义：其编码embedding空间中的特征之间的二阶交互。
    - 优点：这个操作没有引入额外模型参数，且可以在线性时间内计算。

    ![](http://ot0qvixbu.bkt.clouddn.com/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180921180716.png)

  - 隐藏层：全连接，学习特征的高阶交互。

  - 预测层：最后是线性的，无sigmoid和softmax

- 与FM关系：FM是一种浅层、线性的模型，可以看作NFM的没有隐藏层的特例。将隐藏层高度设置为0，直接将Bi-Interaction pooling 输出映射为预测值，将这个模型记为NFM-0，通过将h设置为一个常数向量(1,……,1)，可以得到一个FM模型。

![](http://ot0qvixbu.bkt.clouddn.com/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180925162517.png)

- 拓展：可以在Bi-Interaction层加入dropout。
- 与Wide&Deep、DeepCross的关系：略，见论文。
- 时间复杂度：略，见论文

## 3.2学习过程

- 目标函数：
  - 回归：平方损失
  - 分类：hinge损失或log损失
  - 排序： pairwise personalized ranking损失或contrastive max-margin损失
- 学习算法：SGD（Adagrad）。学习率可以在训练阶段自我调整，这减轻了选择正确学习率的痛苦，并导致比vanilla SGD更快的收敛。
- dropout：
  - 在Bi-Interaction层上采用，获取fBI(Vx)后随机丢弃ρ。由于没有隐藏层的NFM就成为了FM，因此可以将这个操作视为正则化FM的新方法。
  - 在每个隐藏层上使用。
- BN:
  - dnn训练的一个难点就是协方差飘移（covariance shift）。这意味着每个层输入的分布在训练期间会发生变化（因为前一个层的参数会发生变化）。 结果，后一层需要在更新其参数时适应这些变化（噪声很大），这会不利地减慢训练速度。
  - 在Bi-Interaction层上采用，避免输入到隐藏层或预测层的数据分布被特征embedding改变。
  - 在每个隐藏层上使用。

# 4.实验

- 略

# 5.结论&未来工作

- 这项工作是弥合线性模型和深度学习之间差距的第一步。
- 更深层次的模型并不一定会导致好的结果，因为更深层次的模型不太透明，更难以优化和调整。因此，我们预计未来对IR深度学习的研究应该更多地关注设计神经元件或特定任务的架构，而不是依靠更深层次的模型进行微小改进。
- 将来，我们将通过使用散列技术[32,41]来提高NFM的效率，使其更适合大规模应用，并研究其其他IR任务的性能，例如搜索排名和目标广告。

# 一些问题

- 为什么 要从这样的稀疏数据中学习，重要的是要考虑特征之间的相互作用
- LR是线性的？
- FM可以模仿许多特定的分解模型？

- embedding是怎么来的？见[27]
- 协方差飘移？见[20]