---
    author: LuckyGong
    comments: true
    date: 2018-11-30 20:27
    layout: post
    title: Paper-Convolutional Neural Networks for Sentence Classification
    categories:
    - nlp
    tags:
    - nlp
    - paper
---

# Abstract

- 本文基于一些预训练好的词向量上做一些CNN句子分类任务实验。
- 用一个简单的CNN，具有很少的超参结合静态词向量，在多个baseline测试中结果都不错。
- 通过细粒度的调节参数学习特定任务的词向量可以进一步的提升结果。我们提出了对架构简单的修改，这样可以同时使用基于特定任务和静态的词向量

# 1.Introduction

- 通过隐藏层将单词从稀疏的、one-hot编码投影到低维空间上的单词向量本质上是对其单词的语义特征进行编码，可以看作特征提取器。在这种密集的表示中，在较低维向量空间中，语义上接近的单词同样是近距离的欧几里德或余弦距离。
- CNN对NLP有效，在语义分析、信息检索、句子建模、其他NLP任务上有效。
- 本文：
  - 训练一个简单的CNN，在词向量上有一层卷积。
    - 保持预训练词向量不变，效果不错，这表明预先训练的向量是“通用”的特征抽取器，可以用于各种分类任务。
    - 预训练词向量随着训练变动
  - 描述了对体系结构的简单修改，以允许通过多个通道同时使用预先培训的和特定于任务的向量。
  - 我们的工作在哲学上类似于Razavian等人（2014年），该研究表明，对于图像分类，从预训练的深度学习模型中获得的特征抽取器在各种任务上都能很好地执行，包括与接受特征抽取器训练的原始任务非常不同的任务。

# 2.Model

## 2.1网络结构

![](https://images2018.cnblogs.com/blog/890856/201805/890856-20180526154203516-1541380653.png)

- 输入：x_i∈R^k表示句子中第i个词的k维词向量。一个包含n个单词的句子（必要的时候可以padding）可以表示成：x_(1:n)=x_1⊕x_2⊕…⊕x_n，这里⊕表示拼接操作
- 卷积：
  - w∈R^hk代表滤波器，h是词的个数，即单词的窗口大小。
  - c_i=f(w·x _(i:i+h-1)+b)是从单词窗口x _(i:i+h-1)产生的特征c_i，其会组成一列特征。
- 池化：对特征映射采用最大池化策略，即取最大的值作为对应此滤波器的特征。此思路是去捕获最重要的特征——每个特征映射中最大的值。最大池化可以自然处理不同的句子长度， 解决了每个feature map不等长。 
- 全连接：使用多个滤波器（不同的窗口大小）的模型可以获取多个特征。这些特征组成了倒数第二层并且传给全连接的softmax层，输出标签的概率分布。
- 多通道：在其中一个模型变体中，我们实验了两个词向量“通道”，一个在整个训练过程中保持静态，另一个通过反向传播进行微调。在上图所示的多通道体系结构中，每个滤波器都应用于两个通道，并将结果添加到等式（2）中计算，该模型在其他方面相当于单通道体系结构。 

## 2.2正则化

- 在倒数第二层用dropout，公式：y=w∙(z ○r)+b， 这里○是元素级的乘法操作并且r∈R^m是一个“掩盖”向量，向量中的元素都是一个伯努利随机变量有p的概率变为1。梯度仅仅可以通过非掩盖的单元反向传播。在测试阶段，权重向量通过因子p缩减例如w ̂=pw,并且w ̂被用来（没有使用dropout）给看不见的句子打分。这个会提升2-4个百分点。
- 在倒数第二层限制权重向量的二范式，在每一步梯度下降之后，如果‖w‖ _2>s，重新将w的二范式设置为‖w‖ _2=s。

# 3.数据集和实验步骤

## 3.1超参数：

- 用ReLU
- 滤波器窗口3、4、5，每个100种特征图
- dropout率为0.5
- l2参数为3
- batchsize为50
- 从训练数据集中随机选择10%的数据作为验证集
- 用Adadelta进行sgd
- 这些参数是通过网格调参得出的，用SST-2 dev set 
- 交叉验证、未知词向量的初始化、CNN参数的初始化
- 使得随机初始化的向量与预先训练的向量具有相同的方差。

## 3.2预训练的词向量

- 我们使用公开提供的word2vec向量，这些向量是通过Google新闻中的1000亿个单词进行训练的。
- 这些载体的维数为300，并使用连续的词袋结构进行训练。
- 预训练的词中不存在的词被随机初始化。

## 3.3模型变种

- CNN-rand：所有的词向量被随机初始化，并在训练的工程中进行调节。
- CNN-static：使用预训练的词向量Word2vec。所有的词——包括随机初始化的未出现在预训练词向量中的词——保持不变仅仅调节模型其它的参数。
- CNN-non-static：和CNN-static相似，但是预训练的词向量在每个任务中被细粒度的调节
- CNN-multichannel：有两个词向量集合的模型。将每个向量集合看作一个“通道”并且每个滤波器应用在所有的通道，但是梯度只能通过其中一个通道进行反向传播。因此，模型能够细粒度的调节其中一个向量集合，而保持另外一个不变。可以这么理解：将CNN的RGB三通道改为两通道。 

# 4.实验结果

- 结果表明，预训练的矢量是良好的“通用”特征提取器，可以跨数据集使用。 对每项任务的预训练向量进行细化，可以进一步改进（CNN-non-static）。

## 4.1 多通道与单通道模型比较
我们一开始期望多通道的架构可以避免过拟合（通过确保学到词向量不会偏离初始值太远），比单通道的模型效果好，尤其在更小的数据集上。结果，然而是混合的，进一步在规范化细粒度调节过程的工作是必要的。例如，不使用额外的通道作为non-static的部分，也可以使用单通道，在训练过程中训练额外的维度。 
##  4.2 静态与非静态表示比较
正如单通道非静态模型，多通道模型可以细粒度的调节非静态通道使它对手头的任务更具体。例如，在Word2vec中good和bad非常相似，大概是因为他们几乎语义相等。但是在SST-2数据集上，通过非静态通道调节的向量并不是这种情况。相似的，可以说在表达情感上，nice和great相比，与good更相近，这在训练的词向量中被真实的反映出来。 
对于（随机初始化）不在预训练向量集合中的词，细粒度的调节允许它们学到更有意义的表示：网络训练得到感叹号经常与热情洋溢的表达联系在一起，并且逗号经常和连接副词联系在一起。 

## 4.3更多结论

- Adadata（Zeiler，2012）给出了与Adagrad类似的结果（Duchi等人，2011年），但所需时间较少。
- 我们简短地试验了由collobert等人训练的另一组公开可用的词向量。（2011,维基百科做语料），发现word2vec的表现远远优越。
- 当随机初始化不在word2vec中的单词时，我们通过从选择a的u[−a；a]中对每个维度进行抽样，从而获得轻微的改进，使得随机初始化的向量与预先训练的向量具有相同的方差。有趣的是，如果使用更复杂的方法来镜像初始化过程中预先培训过的向量的分布，会有进一步的改进。
- Kalchbrenner等人（2014）向CNN报告更糟糕的结果，CNN的架构与我们的单通道模型基本相同。例如，他们的最大TDNN（时间延迟神经网络）与随机初始化的词获得37:4%的SST-1数据集，相比之下，我们的模型45:0%。我们将这种差异归因于我们的CNN有更多的容量（多个filter宽度和feature map）。

# 5.结论

在本文中，我们描述了一系列基于word2vec的卷积神经网络实验。尽管很少调整超参数，一个简单的CNN与一层卷积的表现非常好。我们的研究结果进一步证实了无监督的词载体预训练是NLP深入学习的重要组成部分。
