---
    author: LuckyGong
    comments: true
    date: 2018-11-30 20:27
    layout: post
    title: Paper-Convolutional Neural Networks for Sentence Classification
    categories:
    - nlp
    tags:
    - nlp
    - paper
---

# Abstract

- 本文基于一些预训练好的词向量上做一些CNN句子分类任务实验。
- 用一个简单的CNN，具有很少的超参结合静态词向量，在多个baseline测试中结果都不错。
- 通过细粒度的调节参数学习特定任务的词向量可以进一步的提升结果。我们提出了对架构简单的修改，这样可以同时使用基于特定任务和静态的词向量

# 1.Introduction

- 通过隐藏层将单词从稀疏的、one-hot编码投影到低维空间上的单词向量本质上是对其单词的语义特征进行编码，可以看作特征提取器。在这种密集的表示中，在较低维向量空间中，语义上接近的单词同样是近距离的欧几里德或余弦距离。
- CNN对NLP有效，在语义分析、信息检索、句子建模、其他NLP任务上有效。
- 本文：训练一个简单的CNN，在词向量上有一层卷积。

# 2.Model

## 2.1网络结构

![](https://images2018.cnblogs.com/blog/890856/201805/890856-20180526154203516-1541380653.png)

- 输入：x_i∈R^k表示句子中第i个词的k维词向量。一个包含n个单词的句子（必要的时候可以padding）可以表示成：x_(1:n)=x_1⊕x_2⊕…⊕x_n，这里⊕表示拼接操作
- 卷积：
  - w∈R^hk代表滤波器，h是词的个数，即单词的窗口大小。
  - c_i=f(w·x _(i:i+h-1)+b)是从单词窗口x _(i:i+h-1)产生的特征c_i，其会组成一列特征。
- 池化：对特征映射采用最大池化策略，即取最大的值作为对应此滤波器的特征。此思路是去捕获最重要的特征——每个特征映射中最大的值。最大池化可以自然处理不同的句子长度， 解决了每个feature map不等长。 
- 全连接：使用多个滤波器（不同的窗口大小）的模型可以获取多个特征。这些特征组成了倒数第二层并且传给全连接的softmax层，输出标签的概率分布。 

## 2.2正则化

- 在倒数第二层用dropout，公式：y=w∙(z ○r)+b， 这里○是元素级的乘法操作并且r∈R^m是一个“掩盖”向量，向量中的元素都是一个伯努利随机变量有p的概率变为1。梯度仅仅可以通过非掩盖的单元反向传播。在测试阶段，权重向量通过因子p缩减例如w ̂=pw,并且w ̂被用来（没有使用dropout）给看不见的句子打分。
- 在倒数第二层限制权重向量的二范式，在每一步梯度下降之后，如果‖w‖ _2>s，重新将w的二范式设置为‖w‖ _2=s。

# 3.数据集和实验步骤

## 3.1超参数：

- 用ReLU
- 滤波器窗口3、4、5，每个100种特征图
- dropout率为0.5
- l2参数为3
- batchsize为50
- 从训练数据集中随机选择10%的数据作为验证集
- 用Adadelta进行sgd

## 3.2预训练的词向量

## 3.3模型变种

- CNN-rand：所有的词向量被随机初始化，并在训练的工程中进行调节
- CNN-static：：使用预训练的词向量Word2vec。所有的词——包括随机初始化的未出现在预训练词向量中的词——保持不变仅仅调节模型其它的参数。
- CNN-non-static：和CNN-static相似，但是预训练的词向量在每个任务中被细粒度的调节
- CNN-multichannel：有两个词向量集合的模型。将每个向量集合看作一个“通道”并且每个滤波器应用在所有的通道，但是梯度只能通过其中一个通道进行反向传播。因此，模型能够细粒度的调节其中一个向量集合，而保持另外一个不变。可以这么理解：将CNN的RGB三通道改为两通道。 

# 4.实验结果

## 4.1 多通道与单通道模型比较 
我们一开始期望多通道的架构可以避免过拟合（通过确保学到词向量不会偏离初始值太远），并且比单通道的模型效果好，尤其在更小的数据集上。结果，然而是混合的，进一步在规范化细粒度调节过程的工作是必要的。例如，不使用额外的通道作为non-static的部分，也可以使用单通道，在训练过程中训练额外的维度。 
##  4.2 静态与非静态表示比较 
正如单通道非静态模型，多通道模型可以细粒度的调节非静态通道使它对手头的任务更具体。例如，在Word2vec中good和bad非常相似，大概是因为他们几乎语义相等。但是在SST-2数据集上，通过非静态通道调节的向量并不是这种情况。相似的，可以说在表达情感上，nice和great相比，与good更相近，这在训练的词向量中被真实的反映出来。 
对于（随机初始化）不在预训练向量集合中的词，细粒度的调节允许它们学到更有意义的表示：网络训练得到感叹号经常与热情洋溢的表达联系在一起，并且逗号经常和连接副词联系在一起。 

# 5.结论



# 