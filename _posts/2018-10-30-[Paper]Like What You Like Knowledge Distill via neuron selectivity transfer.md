# Abstract

- DNN的高性能是以高存储和计算成本为代价的。
- 本文提出了一个新的知识迁移方法：
  - 我们将知识迁移看作是一个分布匹配问题（distribution matching problem）。匹配教师和学生网络之间的神经元选择性模式的分布。
  - 通过最小化这些分布之间的最大均值差异（MMD）来度量设计新的损失函数。

# 1.Introduction

- Hinton的KD方法只能用于具有softmax损失函数的分类任务，且依赖于类的数量（比如二分类问题中KD很难用，因为几乎不能提供额外监督）。
- 本文中：
  - 利用神经元的选择性知识，每个神经元从原始输入中提取与目标相关的特定模式，因此如果在某些区域（regions）或样本（samples）中激活神经元，则意味着这些区域或样本共享着与任务相关的一些共同属性。
  - 这种聚类知识对于学生网络是有价值的，因为它提供了对教师模型的最终预测的解释。
  - 因此，我们建议在学生模型和教师模型之间调整神经元选择性知识的分布。
- 本文贡献：
  - 提供了一个新的知识迁移视角，提出了一种新方法“神经元选择性转移”（NST）。
  - 实验证明NST比学生网络提高了性能。
  - 新方法可以与其他知识迁移方法相结合。

# 2.相关工作

- 域适应（Domain adaptation）：
  - 属于迁移学习。由于目标域没有可用的标签，域自适应的核心是测量和减少两个域（目标域 & 源域）的分布的差异。最大均值差异（MMD）是一种广泛使用的标准，它比较了再生核希尔伯特空间（RKHS）中的分布。
  - 域适应不限于监督学习，……。
- KD：onehot标签旨在将每个类中的样品投影到标签空间中的单个点，而软化目标标签将样品投影到连续分布中。
- Fitnet：FitNet使学生模仿教师的完整特征地图。 然而，由于教师和学生的能力可能差别很大，因此这些假设过于严格

# 3.Background

## 3.1符号

- T：教师网络
- S：学生网络
- RC×HW ：C个通道，尺寸为H * W的输出特征图
- 每个通道的特征图F的每一行表示为：f^K·∈R^HW，每一列表示为：f^·K∈R^HW
- FT和FS代表某些层的教师或学生网络的特征图。 在不失一般性的情况下，我们假设FT和FS具有相同的空间维度。 如果要素图的尺寸不匹配，则可以对其进行插值。

## 3.2MMD（最大均值差）

- 可以看作是基于采样数据的概率分布的距离度量。
- 公式：
  - p、q：两个不同分布
  - X、Y：两个分布中采样得到的数据
  - ϕ：某个映射函数
  - k：某个核函数，k(x,y)=ϕ(x)^Tϕ(y)

$$
\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \Vert \frac{1}{N}\sum_{i=1}^{N}\phi(x^i) - \frac{1}{M}\sum_{j=1}^{M}\phi(y^j) \Vert_2^2\\
= \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}k(x^i, x^j) + \frac{1}{M^2}\sum_{i=1}^{M}\sum_{j=1}^{M}k(y^i, y^j) - \frac{2}{MN}\sum_{i=1}^{N}\sum_{j=1}^{M}k(x^i, y^j)
$$

- 由于MMD损失为0当且仅当特征空间对应域通用RKHS时p=q，最小化MMD相当于最小化p和q之间的距离。

# 4.神经元选择性转移NST

## 4.1Motivation

- 优点：
  - 神经元具有很强的选择性：即什么样的输入可以激发神经元，比如论文中的左图中的神经元对猴脸敏感。
  - 来自神经元的具有高激活的区域可以共享一些与任务相关的相似性，即使这些相似性对于人类解释可能不直观。
  - 为了捕捉这些相似性，学生网络中的神经元也应该模仿这些教师为了神经元的激活模式。所以定义一种新的知识：神经元的选择性（也被称为共激活）。
- 直接匹配特征图有什么缺点：
  - 考虑到将每个空间位置激活为一个特征，则每个滤波器的水平激活图是维度HW的神经元选择性的空间样本。
  - 此样本分布反映了CNN如何解释输入图像：CNN关注的位置在哪里？ CNN强调哪种类型激活模式更多？
  - 对于分布匹配，从它直接匹配样本不是一个好的选择，因为它忽略了空间中的样本密度。 因此，我们采用更先进的分布对齐方法，如下所述。

## 4.2Formulation

- 总体Loss=交叉熵损失+MMD损失
  - H：交叉熵
  - ytrue：真实标签
  - pS：学生网络的输出概率

$$
\mathcal{L} = \mathcal{H}(y_{true}, p_S) + \frac{\lambda}{2}\mathcal{L}_{MMD}(F_T, F_S)
$$

- MMD损失：
  - 用l2归一化版本替换f^k，确保每个样本具有相同比例。
  - 公式：
- 最小化MMD损失相当于将神经元选择性知识从教师迁移到学生。
- Kernel的选择：本文关注以下三种
  - 线性核
  - 多项式核：d=2，c=0
  - 高斯核：σ2 =对的平方距离的均值

## 4.3Discussion

### 4.3.1线性核



### 4.3.2多项式核

# 5.实验

- 数据集：cifar10、cifar100、ImageNet 2012
- 模型：
  - cifar：Resnet-1001->Inception-BN
  - ImageNet：Resnet-101->origial Inception-BN
- 对比：KD、FitNet、AT
- 参数：略

## 5.1cifar

- 图像增强：
  - 对40 * 40的用0填充的图像进行32 * 32的随机裁剪。
  - 对翻转的图像进行32 * 32的随机裁剪。
- 参数：
  - 权重衰减：10^-4
  - batchsize：128
  - epoch：400
  - 学习率：0.2（在200和300epoch时除以10）
- 在Inception-BN的卷积输出in5b和Resnet-1001的最后一个残差模块输出之间加入single transfer loss。