---
    author: LuckyGong
    comments: true
    date: 2019-11-18 20:27
    layout: post
    title: Paper-Deep Neural Networks for YouTube Recommendations
    categories:
    - ad
    tags:
    - ad
    - paper

---



# 1.Introduction

- 主要挑战：
  - 数据量大：需要好的分布式学习算法、服务系统。
  - 冷启动的新内容也要公平推荐（ee视角）
  - 噪音：需要提升模型应对噪音的鲁棒性
    - 由于YouTube上历史用户行为的稀疏性以及一系列不可测外部因素导致预测在本质上是很难的。
    - 很难得知真实的用户满意度，而只能用有噪声的隐式反馈信号替代。
    - 与内容相关联的元数据结构很差，没有明确的定义。
- 本文模型十亿级别参数、千亿样本。NB！

- 相关工作：
  - matrix factorization
  - nn用于新闻推荐
  - nn用于citations
  - nn用于review ratings
  - 协同过滤改为nn、autoencoders
  - nn用于交叉用户建模
  - nn用于音乐推荐



# 2.系统overview

- 召回：一个深度网络
  - 用户历史操作作为输入
  - 通过协同过滤，用户间的相关性由粗略的特征（视频播放ID、query的terms、用户画像）表示。也可以混进来别的召回源。
  - 召回结果要和用户有很高的相关性
  - 召回结果大小为几百
- 排序：一个深度网络
  - 需要高召回率来取top-k
- 评估：精度、召回率、排序loss=>A/B test的ctr、观看时间
- 为什么分为match和rank？match面临百万级视频，单个视频开销必须小，rank算法耗费资源，不能都算一遍。

# 3.召回

- 之前的系统：矩阵分解+rank loss，是浅层网络（这里的dnn可以看做是对mf的一种非线性泛化）。

## 3.1推荐的分类问题

- 公式：

![](.\img\3.png)

- 其中：vi是第i个item的mebedding，u是user的embedding，C是上下文
- label定义：定义隐式feedback为“用户看完了整个视频”。减少长尾问题，避免nn学不到东西。不能用点击率，点击率建模的话，有的正是诱骗进来的，不能表达用户真实喜好。
- 高效的softmax：上百万的类别
  - negative sample：对负样本采样几千个类别，通过importance weighting对此sample进行矫正。
  - 层序softmax：每个节点要做个类别的区分，其实他们类别之间没有相关性的，这使得分类问题更难并且降低性能。
- 在线服务：使用局部敏感hashing算法。在线服务中不用算精确的softmax值， 不需要对来自softmax输出层进行似然校验， 将打分问题转化为在点积空间内的最近邻搜索问题。

## 3.2模型架构

![](.\img\4.png)

- 对于embedding的处理受到了CBOW的启发。



## 3.3异质信号

- 用户浏览过的视频：每个做embedding后求平均，embedding由w2v方法初始化。
- 历史搜索query：先分成unigram或binagram的token，再embedding，embedding由w2v方法初始化。
- 人口统计学信息：很重要：提供先验，对用户冷启动有用。
  - 用户性别、登录状态、年龄：缩放到[0,1]的区间输入到网络中。可以对其进行变换。如对年龄特征进行平方操作，然后作为新的特征。 
  - 用户的地区和设备：embedding
-  Example age：时效性很重要，哪怕牺牲相关性代价，用户还是更倾向于更新的视频。机器学习系统经常会从历史数据学习去预估未来，通常有一个隐偏置/基准（bias）。 视频网站视频的分布是高度非静态（non-stationary）的，但我们的推荐系统产生的视频集合在视频的分布，基本上反映的是训练所取时间段的平均的观看喜好的视频。 为了纠正这一点，在训练的时候我们将『本条记录（example）到现在的时间差』作为一个特征。 在推荐系统提供服务的时候，这个特征设置为零（或绝对值比较小的负数），以反映模型正在训练窗口的末端进行预测。 如果没有这个特征，模型预测所有训练数据窗口的平均情况。

![](.\img\5.png)

## 3.4Label & 上下文

-  使用更广的数据源：不仅仅使用推荐场景的数据进行训练，其他场景比如搜索等的数据也要用到，这样也能为推荐场景提供一些explore，避免过度倾向exploitation。 
- 如果用户通过推荐以外的方式发现了视频，我们想通过协同过滤快速传播这个发现给其他用户
-  对每个用户使用固定的样本数量，这样对每个用户是公平的，防止活跃用户主导了损失函数。
-  抛弃序列信息：我们在实现时尝试的是去掉序列信息，对过去观看视频/历史搜索query的embedding向量进行加权平均。这点其实违反直觉，可能原因是模型对负反馈没有很好的建模。 
-  不对称的共同浏览（asymmetric co-watch）问题：所谓asymmetric co-watch指的是用户在浏览视频时候，往往都是序列式的，开始看一些比较流行的，逐渐找到细分的领域视频。下图所示图(a)是hled-out方式，利用上下文信息预估中间的一个视频；图(b)是predicting next watch的方式，则是利用上文信息，预估下一次浏览的视频。我们发现图(b)的方式在线上A/B test中表现更佳。而实际上，传统的协同过滤类的算法，都是隐含的采用图(a)的held-out方式，忽略了不对称的浏览模式。

![](.\img\6.jpg)



## 3.5特征&深度实验

- 有1M视频词表，有1M搜索的token词表，embedding长度256。观看序列长度50，搜索query序列长度50。softmax维度256。
- 几个epoch。
- 网络结构：塔，输入端最宽，每层减半，有几组对照实验：
  - 0层：线性层，将输入映射为256维输出。
  - 1层：256ReLU
  - 2层：512ReLU、256ReLU
  - 3层：1024ReLU、512ReLU、256ReLU
  - 4层：2048ReLU、1024ReLU、512ReLU、256ReLU

# 4.Ranking

![](.\img\8.png)

- 作用：
  - 精准预估（更精细的featur来刻画）
  - 多召回源无法直接比较，用ranking比较。



## 4.1特征表示

- 特征工程：
  - 描述用户与商品本身或相似商品之间交互特征（是否有交互）最好用， 以下两个连续特征的最大好处是具备非常强的泛化能力：
    - 数量特征：浏览该频道的次数？
    - 时间特征：比如最近一次浏览该频道距离现在的时间？
  -  把Matching阶段的信息传播到Ranking阶段同样能很好的提升效果，比如推荐来源和所在来源的分数。 
  -  用户对于视频所在频道的一些PV但不点击的行为，即负反馈Signal同样非常重要。 

- Embedding：

  -  一般来说embedding的维度基本与log(去重后值的数量)相当 
  -  实际并非为所有的id进行embedding，比如视频id，只需要按照点击排序，选择top N视频进行embedding，其余置为0向量。 
  -  同维度不同feature采用的相同ID的embedding是共享的 ，可以加速训练、提升范化、减小内存。

- Continuous Features：

  - NN对连续值的取值区间和分布是很敏感的，决策树对连续值不敏感。

  - 连续值的normalize操作对收敛很重要。使用排序分位归一方法（累积分布函数），积分用线性插值去近似特征值分位。

    ![](.\img\7.png)

  -  我们还把归一化后的x的根号![\sqrt{x} ](http://www.zhihu.com/equation?tex=%5Csqrt%7Bx%7D+)和平方![x^{2} ](http://www.zhihu.com/equation?tex=x%5E%7B2%7D+)作为网络输入，以期能使网络能够更容易得到特征的次线性（sub-linear）和（super-linear）超线性函数。 

## 4.2期望观看时间建模

- 正样本是点击的，负样本是没点击的，正样本有观看时间，负样本没有。

- 网络最后一层使用weighted lr来预估观看时间。正样本权重为Ti，负样本权重为1。则lr的几率为：

  $\frac{\sum T_i}{N-k}，N是训练数据量，k是正样本量，T_i是第i个正样本的观看时长。$

  $由于k<<N，则上式可以转换为\frac{E[T]}{1+P} \approx E[T] ，P是点击率$

- 推理的时候用$e^x$做激励函数，算出了lr的几率，近似地估计了观看时长。反正排序使用的是相对顺序，计算p和计算e^x 不改变相对序。

- 单纯CTR指标是有迷惑性的，有些靠关键词吸引用户高点击的视频未必能够被播放。因此设定的目标基本与期望的观看时长相关，具体的目标调整则根据线上的A/B进行调整。



## 4.3实验

- 每个配置得到的值是通过在同一个页面上展示给一个用户的正负样本而来的。如果负样本的得分比正样本的得分高，就认为这个正样本的观看时长是错误预测。每个用户的损失值就是所有错误预测的观看时长/所有预测的观看时长。
- 神经网络的深度、宽度要和在线的CPU推理时间做trade-off。
  - None
  - 256ReLU
  - 512ReLU
  - 1024ReLU
  - 512ReLU-256ReLU
  - 1024ReLU-512ReLU
  - 1024ReLU-512ReLU-256ReLU

# 5.结论

